import torch
import time
import torch.nn as nn
import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.interpolate import griddata
from utilities import create_save_path, gradients, Sampler, relative_error, mean_squared_error, to_device, wn_linear

torch.manual_seed(1234)
np.random.seed(1234)

# path
u_path = "../input/reference_u.csv"
v_path = "../input/reference_v.csv"
save_path = "../output/NavierStokes2D/EM-RBN/"
# Parameter
Re = 100
# domain
bound_l = 0
bound_r = 1
bound_t = 1
bound_d = 0
# network
layers = [2, 50, 2]
batch_size = 10000
bound_weight = 1
c_x = [bound_l - (bound_r - bound_l)/10, bound_r + bound_r/10]
c_y = [bound_d - (bound_t - bound_d)/10, bound_t + bound_t/10]

# 中心点c： RBF中心点应该均匀分布  选择20*20的网格  在边界外侧增加一层RBF中心点
# b: b=10  每个RBF覆盖至少15个样本点
# a: 可训练参数
class RBF_layer1(nn.Module):
    def __init__(self, n_neu, c, device=None):
        super(RBF_layer1, self).__init__()
        if device is None:
            device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.device = device
        self.n_neu = n_neu
        self.c = c.to(device)  # 移动中心点到 GPU
        self.b = nn.Parameter(torch.randn(1, n_neu, dtype=torch.float32, device=device), requires_grad=True)

    def forward(self, inputs):
        inputs = inputs.to(self.device)
        t2 = (inputs[..., 0, None] ** 2 + inputs[..., 1, None] ** 2)
        D = (self.c[None, 0, :] ** 2 + self.c[None, 1, :] ** 2)
        t1 = (2 * torch.matmul(inputs, self.c))
        return torch.exp((t1 - D - t2) * self.b ** 2)


class PIRBN(torch.nn.Module):
    def __init__(self, X, layers, device):
        super().__init__()
        self.device = device
        self.X_mean = X.mean(0, keepdims=True).to(self.device)
        self.X_std = X.std(0, keepdims=True).to(self.device)
        self.n_in = layers[0]
        self.n_out = layers[-1]
        self.n_neu_x = 61  # RBF 网络在 x 和 y 方向上的神经元数量
        self.n_neu_y = 61
        self.b = 20  # 较大的计算域需要较小的b 源码中[-1.1, 1.1]对应的b=20 [-Π,Π] b=3.8
        self.c, self.inputs_len = self.reviseC(c_x, c_y)  # RBF中心点c和其长度

        self.layers = nn.ModuleList()
        self.layers.append(RBF_layer1(self.inputs_len, self.c, device=self.device))
        self.layers.append(nn.Linear(self.inputs_len, self.n_out, bias=False))
        self.layers = nn.Sequential(*self.layers)
        self.init_weights(self.layers)

    def reviseC(self, c_x, c_y):
        # RBF中心点坐标
        c = np.zeros((2, self.n_neu_x * self.n_neu_y)).astype(dtype='float32')
        k = 0
        # x 和 y 方向上相邻中心点之间的间隔
        dx = (c_x[1] - c_x[0]) / (self.n_neu_x - 1)
        dy = (c_y[1] - c_y[0]) / (self.n_neu_y - 1)
        # 遍历网格中的每个位置 计算对应的中心点坐标
        for i in range(self.n_neu_x):
            for j in range(self.n_neu_y):
                c[0, k] = i * dx + c_x[0]
                c[1, k] = j * dy + c_y[0]
                k = k + 1

        length = c.shape[1]
        c = torch.tensor(c, device=self.device)
        return c, length

    def init_weights(self, layers):
        b = (np.ones((1, self.inputs_len)) * self.b).astype(np.float32)
        b = torch.from_numpy(b).to(self.device).requires_grad_(True)
        layers[0].b.data = b

    def forward(self, x):
        x = x.to(self.device)
        y = self.layers(x)
        return y


class ExpandingLayer(nn.Module):
    def __init__(self):
        super(ExpandingLayer, self).__init__()

    def forward(self, x):
        # 计算各项特征
        pow_xt = torch.pow(x, 2)  # 平方(x^2, t^2)
        sin_xt = torch.sin(x)  # 正弦(sin(x), sin(t))
        cos_xt = torch.cos(x)  # 余弦(cos(x), cos(t))
        # 拼接所有特征
        out = torch.cat([pow_xt, sin_xt, cos_xt, x], dim=1)
        return out


class DFSModified(torch.nn.Module):
    def __init__(self, X, layers, device):
        super().__init__()
        # initial mean and std
        self.X_mean = X.mean(0, keepdims=True).to(device)
        self.X_std = X.std(0, keepdims=True).to(device)
        self.expanding = ExpandingLayer()
        layers = [8] + layers[1:]  # 第一层输入改为8
        self.u = wn_linear(layers[0], layers[1]).to(device)
        self.v = wn_linear(layers[0], layers[1]).to(device)
        # build network
        self.fnn = torch.nn.Sequential().to(device)
        for i in range(len(layers) - 2):
            self.fnn.add_module('dense_%d' % i, wn_linear(layers[i], layers[i + 1]).to(device))
            self.fnn.add_module('tanh_%d' % i, torch.nn.Tanh())
        self.fnn.add_module('dense_output', wn_linear(layers[-2], layers[-1]).to(device))

    def forward(self, X):
        H = (X - self.X_mean) / self.X_std
        H = self.expanding(H)
        u = torch.nn.Tanh()(self.u(H))
        v = torch.nn.Tanh()(self.v(H))
        for i in range(0, len(self.fnn) - 2, 2):
            linear_layer = self.fnn[i]
            activation_layer = self.fnn[i + 1]
            H = linear_layer(H)
            H = activation_layer(H)
            H = H * u + (1 - H) * v

        out = self.fnn[-1](H)
        return out


class CombinedNetwork(torch.nn.Module):
    def __init__(self, X, layers, device=None, alpha=0.5):
        super(CombinedNetwork, self).__init__()
        if device is None:
            device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.device = device
        self.pirbn = PIRBN(X, layers, device=device)
        self.dfsnet = DFSModified(X, layers, device=device)
        # 将alpha转换为可训练参数，使用sigmoid约束在[0,1]范围内
        self.raw_alpha = nn.Parameter(torch.tensor([self.inverse_sigmoid(alpha)],dtype=torch.float32, requires_grad=True, device=self.device))
        self.temperature = nn.Parameter(torch.tensor(1.0), requires_grad=False).to(device)
        # Small network to potentially make alpha input-dependent
        self.alpha_network = nn.Sequential(
            nn.Linear(layers[0], 16),
            nn.Tanh(),
            nn.Linear(16, 1),
            nn.Sigmoid()
        ).to(self.device)
        self.use_input_dependent_alpha = True  # Can be set to True for more complex weighting

    def inverse_sigmoid(self, x):
        return torch.log(torch.tensor(x / (1 - x + 1e-10)))

    def forward(self, inputs):
        pirbn_out = self.pirbn(inputs)
        dfsnet_out = self.dfsnet(inputs)
        if self.use_input_dependent_alpha:
            alpha = self.alpha_network(inputs)
        else:
            alpha = torch.sigmoid(self.raw_alpha * self.temperature)
        alpha = torch.clamp(alpha, 0.01, 0.99)
        return alpha * pirbn_out + (1 - alpha) * dfsnet_out


class DatasetNavierStokes2D:
    def __init__(self, Re, bound_l, bound_r, bound_t, bound_d, u_path, v_path, device="cuda:0"):
        # Parameter
        self.Re = Re
        self.bound_l = bound_l
        self.bound_r = bound_r
        self.bound_t = bound_t
        self.bound_d = bound_d
        self.device = torch.device(device)

        # Domain boundaries
        bc1_coords = np.array([[self.bound_l, self.bound_t], [self.bound_r, self.bound_t]])
        bc2_coords = np.array([[self.bound_l, self.bound_d], [self.bound_l, self.bound_t]])
        bc3_coords = np.array([[self.bound_r, self.bound_d], [self.bound_r, self.bound_t]])
        bc4_coords = np.array([[self.bound_l, self.bound_d], [self.bound_r, self.bound_d]])

        dom_coords = np.array([[self.bound_l, self.bound_d], [self.bound_r, self.bound_t]])

        # Create boundary conditions samplers
        self.bc1_sampler = Sampler(2, bc1_coords, lambda x: self.U_gamma_1(x), name='Dirichlet BC1')
        self.bc2_sampler = Sampler(2, bc2_coords, lambda x: self.U_gamma_2(x), name='Dirichlet BC2')
        self.bc3_sampler = Sampler(2, bc3_coords, lambda x: self.U_gamma_2(x), name='Dirichlet BC3')
        self.bc4_sampler = Sampler(2, bc4_coords, lambda x: self.U_gamma_2(x), name='Dirichlet BC4')

        # Create residual sampler
        self.pde_sampler = Sampler(2, dom_coords, lambda x: self.f(x), name='Forcing')

        # test data
        self.u_ref = np.genfromtxt(u_path, delimiter=',')
        self.v_ref = np.genfromtxt(v_path, delimiter=',')
        self.velocity_ref = np.sqrt(self.u_ref ** 2 + self.v_ref ** 2).T

    def U_gamma_1(self, x):
        # boundary conditional 1
        num = x.shape[0]
        return np.tile(np.array([1.0, 0.0]), (num, 1))

    def U_gamma_2(self, x):
        # boundary conditional 2
        num = x.shape[0]
        return np.zeros((num, 2))

    def f(self, x):
        # Forcing term
        num = x.shape[0]
        return np.zeros((num, 2))

    def residual(self, X, PSI):
        U = gradients(PSI[:, 0], X)
        u = U[:, 1]
        v = -U[:, 0]
        u_X = gradients(u, X)
        u_x = u_X[:, 0]
        u_y = u_X[:, 1]
        v_X = gradients(v, X)
        v_x = v_X[:, 0]
        v_y = v_X[:, 1]
        p_X = gradients(PSI[:, 1], X)
        p_x = p_X[:, 0]
        p_y = p_X[:, 1]
        u_xX = gradients(u_x, X)
        u_xx = u_xX[:, 0]
        u_yX = gradients(u_y, X)
        u_yy = u_yX[:, 1]
        v_xX = gradients(v_x, X)
        v_xx = v_xX[:, 0]
        v_yX = gradients(v_y, X)
        v_yy = v_yX[:, 1]
        Ru_momentum = u * u_x + v * u_y + p_x - (u_xx + u_yy) / self.Re
        Rv_momentum = u * v_x + v * v_y + p_y - (v_xx + v_yy) / self.Re
        return Ru_momentum, Rv_momentum

    def get_bound_batch(self, N):
        N = N // 4
        # sample points from boundary
        bc1_x, bc1_u = self.bc1_sampler.sample(N)
        bc2_x, bc2_u = self.bc2_sampler.sample(N)
        bc3_x, bc3_u = self.bc3_sampler.sample(N)
        bc4_x, bc4_u = self.bc4_sampler.sample(N)
        # rearrange data
        bc_x = np.concatenate([bc1_x, bc2_x, bc3_x, bc4_x])
        bc_u = np.concatenate([bc1_u, bc2_u, bc3_u, bc4_u])
        return (torch.tensor(bc_x, dtype=torch.float32).to(self.device),
                torch.tensor(bc_u, dtype=torch.float32).to(self.device))

    def get_pde_batch(self, N):
        # sample points from training domain, residual = 0
        pde_x, _ = self.pde_sampler.sample(N)
        return torch.tensor(pde_x, dtype=torch.float32).to(self.device)

    def get_test_batch(self, N):
        # sample batch for test
        # X_star size = [N, 1], velocity_ref size = [N, 1]
        nn = int(N ** (0.5))
        # generate mesh
        x = np.linspace(self.bound_l, self.bound_r, nn)[:, None]
        y = np.linspace(self.bound_d, self.bound_t, nn)[:, None]
        x, y = np.meshgrid(x, y)
        # sample X and V
        X_star = np.hstack((x.flatten()[:, None], y.flatten()[:, None]))
        velocity_ref = self.velocity_ref.flatten()[:, None]
        return (torch.tensor(X_star, dtype=torch.float32).to(self.device),
                torch.tensor(velocity_ref, dtype=torch.float32).to(self.device))

    def get_img_batch(self, N):
        # sample batch for create image
        # X_star size = [sqrt(N), sqrt(N)], velocity_ref size = [sqrt(N), sqrt(N)]
        nn = int(N ** (0.5))
        # generate mesh
        x = np.linspace(self.bound_l, self.bound_r, nn)[:, None]
        y = np.linspace(self.bound_d, self.bound_t, nn)[:, None]
        x, y = np.meshgrid(x, y)
        # rearrange data
        X_star = np.hstack((x.flatten()[:, None], y.flatten()[:, None]))
        return (torch.tensor(x, dtype=torch.float32).to(self.device),
                torch.tensor(y, dtype=torch.float32).to(self.device),
                torch.tensor(X_star, dtype=torch.float32).to(self.device),
                torch.tensor(self.velocity_ref, dtype=torch.float32).to(self.device))


class PINNNavierStokes2D():
    def __init__(self, network, dataset, batch_size=10000, bound_weight=1, log_path=None,
                 model_path=None, pic_path=None, device="cuda:0"):
        self.network = network.to(device)  # 确保网络在指定设备上
        self.dataset = dataset
        self.batch_size = batch_size
        self.bound_weight = bound_weight
        self.log_path = log_path
        self.model_path = model_path
        self.pic_path = pic_path
        self.device = torch.device(device)

    def train(self, lr_rate, decay_factor, total_it, print_it, evaluate_it, pic_it, lr_it):
        # optimizer
        optimizer = torch.optim.Adam(self.network.parameters(), lr=lr_rate)
        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, decay_factor)

        self.logging('\nstart training...')
        tic = time.time()
        min_l2 = 99999
        time1 = time.time()

        # once iteration
        for it in range(total_it):
            # loss function
            loss_pde = self.pde_loss()
            loss_bound = self.bound_loss()
            loss = loss_pde + self.bound_weight * loss_bound

            # backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if it % print_it == 0:
                self.logging(f'it {it}: loss {loss:.3e}, loss pde {loss_pde:.3e}, '
                             f'loss bc {loss_bound:.3e}')

            # evaluate
            if it and it % evaluate_it == 0:
                time2 = time.time()
                self.logging(f'training time is:{time2 - time1}')
                time1 = time2
                l2_error = self.evaluation()

                # save model
                if l2_error < min_l2:
                    min_l2 = l2_error
                    self.save_model(it, min_l2)

            # save picture
            if it and it % pic_it == 0:
                self.save_pic(it)

            # update learning rate
            if it and it % lr_it == 0:
                scheduler.step()
                self.logging("Update learning rate, lr: %2e" % scheduler.get_last_lr()[0])

        toc = time.time()
        self.logging(f'total training time: {toc - tic}')

    def pde_loss(self):
        # get mini batch in domain
        X_pde = self.dataset.get_pde_batch(self.batch_size)
        # pred U and compute residual loss
        X_pde = X_pde.to(self.device).requires_grad_(True)
        PSI_pred = self.network(X_pde)
        Ru_momentum, Rv_momentum = self.dataset.residual(X_pde, PSI_pred)
        residual = torch.abs(Ru_momentum) + torch.abs(Rv_momentum)
        return mean_squared_error(Ru_momentum, torch.zeros_like(Ru_momentum).to(self.device)) + \
            mean_squared_error(Rv_momentum, torch.zeros_like(Rv_momentum).to(self.device))

    def PSI2U(self, PSI, X):
        # 确保 PSI 和 X 支持梯度计算，并在 GPU 上
        PSI = PSI.requires_grad_(True).to(self.device)
        X = X.requires_grad_(True).to(self.device)
        # convect potential function PSI into physics field U
        U_pred = gradients(PSI[:, 0], X)
        u = U_pred[:, 1:2]
        v = -U_pred[:, 0:1]
        U = torch.concatenate((u, v), -1)
        return U.to(self.device)  # 确保返回值在 device 上

    def bound_loss(self):
        # get mini batch in boundary
        X_bound, U_bound = self.dataset.get_bound_batch(self.batch_size)

        # pred potential function PSI, and convect into
        X_bound = to_device(X_bound, self.device)
        U_pred = self.predict(X_bound)
        return mean_squared_error(U_pred, U_bound.to(self.device))

    def predict(self, X):
        # directly predicting physics fields
        X = X.to(self.device)
        PSI_pred = self.network(X)
        U_pred = self.PSI2U(PSI_pred, X)
        return U_pred

    def evaluation(self):
        self.network.eval()

        # get mini batch for test
        X_test, velocity_ref = self.dataset.get_test_batch(self.batch_size)

        # predict U
        X_test = to_device(X_test, self.device)
        U_pred = self.predict(X_test)

        # convect into velocity
        u_pred = U_pred[:, 0:1]
        v_pred = U_pred[:, 1:2]
        velocity_pred = torch.sqrt(u_pred ** 2 + v_pred ** 2)

        # compute l2 error
        error = relative_error(velocity_pred, velocity_ref.to(self.device))
        self.logging(f'l2 related error: {error:.3e}')

        self.network.train()
        return error.item()

    def logging(self, log_item):
        # write into consolo and file
        with open(self.log_path, 'a+') as log:
            log.write(log_item + '\n')
        print(log_item)

    def save_model(self, step, l2):
        torch.save({'step': step, 'model': self.network.state_dict(), 'l2_error': l2}, self.model_path)
        log_item = "Model checkpoint successful saved in %s" % self.model_path
        self.logging(log_item)

    def save_pic(self, it):
        # get image data
        x1, x2, X_star, velocity_ref = self.dataset.get_img_batch(self.batch_size)
        # 确保数据在 GPU 上
        x1 = x1.to(self.device).cpu().numpy()  # 将 x1 移动到 GPU 再转为 NumPy 数组
        x2 = x2.to(self.device).cpu().numpy()  # 将 x2 移动到 GPU 再转为 NumPy 数组
        X_star = X_star.to(self.device).cpu().numpy()  # 将 X_star 移动到 GPU 再转为 NumPy 数组
        velocity_ref = velocity_ref.to(self.device).cpu().numpy()  # 将 velocity_ref 移动到 GPU 再转为 NumPy 数组
        # 确保 X_star_tensor 设置 requires_grad=True 并在 GPU 上
        X_star_tensor = torch.tensor(X_star, dtype=torch.float32, requires_grad=True).to(self.device)

        # 预测 U
        U_pred = self.predict(X_star_tensor)
        # 将预测值从 GPU 移动到 CPU，并分离张量以避免梯度问题
        U_pred = U_pred.detach().cpu().numpy()

        # compute velocity
        velocity = np.sqrt(U_pred[:, 0:1] ** 2 + U_pred[:, 1:2] ** 2)
        velocity_pred = griddata(X_star, velocity.flatten(), (x1, x2), method='cubic')

        plt.figure(1, figsize=(18, 5))

        # draw pic for real velocity
        plt.subplot(1, 3, 1)
        plt.pcolor(x1, x2, velocity_ref, cmap='jet')
        plt.colorbar()
        plt.xlabel(r'$x$')
        plt.ylabel(r'$y$')
        plt.title('Exact $velocity(x)$')

        # draw pic for Predicted velocity
        plt.subplot(1, 3, 2)
        plt.pcolor(x1, x2, velocity_pred, cmap='jet')
        plt.colorbar()
        plt.xlabel(r'$x$')
        plt.ylabel(r'$y$')
        plt.title('Predicted $velocity(x)$')

        # draw pic for Absolute error
        plt.subplot(1, 3, 3)
        plt.pcolor(x1, x2, np.abs(velocity_ref - velocity_pred), cmap='jet')
        plt.colorbar()
        plt.xlabel(r'$x$')
        plt.ylabel(r'$y$')
        plt.title('Absolute error')
        plt.tight_layout()

        # save pictures
        fig_path = "test_%d.jpg" % (it)
        plt.savefig(os.path.join(self.pic_path, fig_path))
        self.logging("Pictures has been successfully saved in %s" % os.path.join(self.pic_path, fig_path))
        plt.close('all')


if __name__ == '__main__':
    # load device
    device = torch.device(f"cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Use GPU: {torch.cuda.is_available()}\n")
    # create save path
    log_path, model_path, pic_path = create_save_path(save_path)
    # load dataset
    dataset = DatasetNavierStokes2D(Re, bound_l, bound_r, bound_t, bound_d, u_path, v_path)
    X_star = dataset.get_pde_batch(10000)
    # create model
    network = CombinedNetwork(X_star, layers, device).to(device)
    pinn = PINNNavierStokes2D(network, dataset, batch_size, bound_weight, log_path, model_path, pic_path, device)
    # train model
    pinn.train(lr_rate=1e-3, decay_factor=0.5, total_it=25010, print_it=10, evaluate_it=100, pic_it=1000, lr_it=10000)
