import torch
import time
import torch.nn as nn
import os
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import griddata
from utilities import create_save_path, gradients, Sampler, to_numpy, relative_error, mean_squared_error, to_device

torch.manual_seed(1234)
np.random.seed(1234)

# Parameter
a = 5
# domain
bound_l = 0
bound_r = 1
bound_t = 1
bound_d = 0
layers = [2, 128, 1]
batch_size = 10000
bound_weight = 1
ic_weight = 1
c_t = [bound_l - (bound_r - bound_l) / 10, bound_r + bound_r / 10]
c_x = [bound_d - (bound_d - bound_t) / 10, bound_t + bound_t / 10]

# 中心点c： RBF中心点应该均匀分布  选择20*20的网格  在边界外侧增加一层RBF中心点
# b: b=10  每个RBF覆盖至少15个样本点
# a: 可训练参数
class RBF_layer1(nn.Module):
    def __init__(self, n_neu, c, device):
        super(RBF_layer1, self).__init__()
        self.n_neu = n_neu  # 神经元数量
        self.c = c.to(device)  # RBF中心点
        self.b = nn.Parameter(torch.randn(1, n_neu, dtype=torch.float32, device=device), requires_grad=True)  # 初始化的网络参数，就是用20替换这个

    def forward(self, inputs):
        t2 = (inputs[..., 0, None] ** 2 + inputs[..., 1, None] ** 2)
        D = (self.c[None, 0, :] ** 2 + self.c[None, 1, :] ** 2)
        t1 = (2 * torch.matmul(inputs, self.c))
        return torch.exp((t1 - D - t2) * self.b ** 2)


class NeuralNetwork(torch.nn.Module):
    def __init__(self, X, layers, device):
        super().__init__()
        self.device = device
        self.X_mean = X.mean(0, keepdims=True).to(self.device)
        self.X_std = X.std(0, keepdims=True).to(self.device)
        self.n_in = layers[0]
        self.n_out = layers[-1]
        self.n_neu_x = 61  # RBF 网络在 x 和 y 方向上的神经元数量
        self.n_neu_y = 61
        self.b = 20  # 较大的计算域需要较小的b 源码中[-1.1, 1.1]对应的b=20 [-Π,Π] b=3.8
        self.c, self.inputs_len = self.reviseC(c_t, c_x)  # RBF中心点c和其长度

        self.layers = nn.ModuleList()
        self.layers.append(RBF_layer1(self.inputs_len, self.c, device=self.device))
        self.layers.append(nn.Linear(self.inputs_len, self.n_out, bias=False))
        self.layers = nn.Sequential(*self.layers)
        self.init_weights(self.layers)

    def reviseC(self, c_x, c_y):
        # RBF中心点坐标
        c = np.zeros((2, self.n_neu_x * self.n_neu_y)).astype(dtype='float32')
        k = 0
        # x 和 y 方向上相邻中心点之间的间隔
        dx = (c_x[1] - c_x[0]) / (self.n_neu_x - 1)
        dy = (c_y[1] - c_y[0]) / (self.n_neu_y - 1)
        # 遍历网格中的每个位置 计算对应的中心点坐标
        for i in range(self.n_neu_x):
            for j in range(self.n_neu_y):
                c[0, k] = i * dx + c_x[0]
                c[1, k] = j * dy + c_y[0]
                k = k + 1

        length = c.shape[1]
        c = torch.tensor(c, device=self.device)
        return c, length

    def init_weights(self, layers):
        b = (np.ones((1, self.inputs_len)) * self.b).astype(np.float32)
        b = torch.from_numpy(b).to(self.device).requires_grad_(True)
        layers[0].b.data = b

    def forward(self, x):
        x = x.to(self.device)
        y = self.layers(x)
        return y

class PINNKG():
    def __init__(self, network, dataset, batch_size=10000, bound_weight=1, ic_weight=1, log_path=None,
                 model_path=None, pic_path=None, device="cuda:0"):
        self.network = network.to(device)
        self.dataset = dataset
        self.batch_size = batch_size
        self.bound_weight = bound_weight
        self.ic_weight = ic_weight
        self.log_path = log_path
        self.model_path = model_path
        self.pic_path = pic_path
        self.device = torch.device(device)

    def train(self, lr_rate, decay_factor, total_it, print_it, evaluate_it, pic_it, lr_it):
        optimizer = torch.optim.Adam(self.network.parameters(), lr=lr_rate)
        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, decay_factor)
        self.logging('\nstart training...')
        tic = time.time()
        min_l2 = 99999
        time1 = time.time()

        for it in range(total_it):
            loss_pde = self.pde_loss()
            loss_bc = self.boundary_loss()
            loss_ic = self.ic_loss()
            loss = loss_pde + self.bound_weight * loss_bc + self.ic_weight * loss_ic

            # backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if it % print_it == 0:
                self.logging(f'it {it}: loss {loss:.3e}, loss_pde {loss_pde:.3e}, '
                             f'loss_bc {loss_bc:.3e}, loss_ic {loss_ic:.3e}')

            # evaluate
            if it and it % evaluate_it == 0:
                time2 = time.time()
                self.logging(f'training time is:{time2 - time1}')
                time1 = time2
                l2_error = self.evaluation()

                # save model
                if l2_error < min_l2:
                    min_l2 = l2_error
                    self.save_model(it, min_l2)

            # save picture
            if it and it % pic_it == 0:
                self.save_pic(it)


            # update learning rate
            if it and it % lr_it == 0:
                scheduler.step()
                self.logging("Update learning rate, lr: %2e" % scheduler.get_last_lr()[0])

        toc = time.time()
        self.logging(f'total training time: {toc - tic}')

    def pde_loss(self):
        # get mini batch in domain
        X_pde, f_pde = self.dataset.get_pde_batch(self.batch_size)
        X_pde = to_device(X_pde, self.device)
        # pred U and compute residual loss
        u_pred = self.network(X_pde)
        residual = self.dataset.residual(X_pde, u_pred)
        return mean_squared_error(residual, f_pde.to(self.device))

    def boundary_loss(self):
        # get mini batch in boundary
        X_bound, u_bound = self.dataset.get_bound_batch(self.batch_size)
        u_pred = self.network(X_bound.to(self.device))
        return mean_squared_error(u_pred, u_bound.to(self.device))

    def ic_loss(self):
        X_ic, u_ic = self.dataset.get_ic_batch(self.batch_size)
        u_pred = self.network(X_ic.to(self.device))
        return mean_squared_error(u_pred, u_ic.to(self.device))

    def predict(self, X):  # prediction with no gradients
        with torch.no_grad():
            Y_pred = self.network(X)
        return Y_pred

    def evaluation(self):
        self.network.eval()
        # get mini batch for test
        X_test, U_test = self.dataset.get_test_batch(self.batch_size)
        # predict U
        U_pred = self.predict(X_test.to(self.device))
        # compute l2 error
        error = relative_error(U_pred, U_test.to(self.device))
        self.logging(f'l2 related error: {error:.3e}')
        self.network.train()
        return error.item()

    def logging(self, log_item):
        # write into consolo and file
        with open(self.log_path, 'a+') as log:
            log.write(log_item + '\n')
        print(log_item)

    def save_model(self, step, l2):
        torch.save({'step': step, 'model': self.network.state_dict(), 'l2_error': l2}, self.model_path)
        log_item = "Model checkpoint successful saved in %s" % self.model_path
        self.logging(log_item)

    def save_pic(self, it):
        # get image data
        x1, x2, X_star, U_star = self.dataset.get_img_batch(self.batch_size)
        # # (100, 100) (100, 100) (10000, 2) (10000, 1)
        # predict U
        U_pred = self.predict(torch.tensor(X_star, dtype=torch.float32).to(self.device))
        # rearrange data
        U_star = griddata(X_star, U_star.flatten(), (x1, x2), method='cubic')
        U_pred = griddata(X_star, to_numpy(U_pred).flatten(), (x1, x2), method='cubic')
        # print(x1.shape, x2.shape, X_star.shape, U_star.shape, U_pred.shape)
        # (100, 100) (100, 100) (10000, 2) (100, 100) (100, 100)
        plt.figure(1, figsize=(18, 5))
        # draw pic for real u
        plt.subplot(1, 3, 1)
        plt.pcolor(x1, x2, U_star, cmap='jet')
        plt.colorbar()
        plt.xlabel(r'$t$')
        plt.ylabel(r'$x$')
        plt.title('Exact $u(t,x)$')
        # draw pic for Predicted u
        plt.subplot(1, 3, 2)
        plt.pcolor(x1, x2, U_pred, cmap='jet')
        plt.colorbar()
        plt.xlabel(r'$t$')
        plt.ylabel(r'$x$')
        plt.title('Predicted $u(t,x)$')

        # draw pic for Absolute error
        plt.subplot(1, 3, 3)
        plt.pcolor(x1, x2, np.abs(U_star - U_pred), cmap='jet')
        plt.colorbar()
        plt.xlabel(r'$t$')
        plt.ylabel(r'$x$')
        plt.title('Absolute error')
        plt.tight_layout()

        # save pictures
        fig_path = "test_%d.jpg" % (it)
        plt.savefig(os.path.join(self.pic_path, fig_path))
        self.logging("Pictures has been successfully saved in %s" % os.path.join(self.pic_path, fig_path))
        plt.close('all')


class DatasetKG:
    def __init__(self, a, bound_l, bound_r, bound_t, bound_d):
        self.a = a
        self.bound_l = bound_l
        self.bound_r = bound_r
        self.bound_t = bound_t
        self.bound_d = bound_d
        # Domain boundaries 下 上 左
        bc1_coords = np.array([[self.bound_l, self.bound_d], [self.bound_r, self.bound_d]])
        bc2_coords = np.array([[self.bound_l, self.bound_t], [self.bound_r, self.bound_t]])
        ic_coords = np.array([[self.bound_l, self.bound_d], [self.bound_l, self.bound_t]])
        dom_coords = np.array([[self.bound_l, self.bound_d], [self.bound_r, self.bound_t]])

        # Create boundary conditions samplers
        self.bc1_sampler = Sampler(2, bc1_coords, lambda x: self.u(x), name='Dirichlet BC1')
        self.bc2_sampler = Sampler(2, bc2_coords, lambda x: self.u(x), name='Dirichlet BC2')
        self.ic_sampler = Sampler(2, ic_coords, lambda x: self.u(x), name='Dirichlet BC3')

        # Create residual sampler
        self.pde_sampler = Sampler(2, dom_coords, lambda x: self.f(x), name='Forcing')

    def u(self, x):
        # ground truth 解析解
        return x[:, 1:2] * np.cos(self.a * np.pi * x[:, 0:1]) + (x[:, 1:2] * x[:, 0:1]) ** 3

    def f(self, x):
        # Forcing term
        u_tt = -x[:, 1:2] * (5 * np.pi) ** 2 * np.cos(5 * np.pi * x[:, 0:1]) + 6 * x[:, 0:1] * x[:, 1:2] ** 3
        u_xx = 6 * x[:, 1:2] * x[:, 0:1] ** 3
        return u_tt - u_xx + self.u(x) ** 3

    def residual(self, X, u):
        u_x = gradients(u, X)[:, 1:2]
        u_t = gradients(u, X)[:, 0:1]
        u_xx = gradients(u_x, X)[:, 1:2]
        u_tt = gradients(u_t, X)[:, 0:1]
        residual = u_tt - u_xx + u ** 3
        return residual

    def get_pde_batch(self, N):
        # sample points from training domain, residual = f
        pde_x, pde_f = self.pde_sampler.sample(N)
        return (torch.tensor(pde_x, dtype=torch.float32),
                torch.tensor(pde_f, dtype=torch.float32))

    def get_bound_batch(self, N):
        N = N // 2
        # sample points from boundary
        bc1_x, bc1_u = self.bc1_sampler.sample(N)
        bc2_x, bc2_u = self.bc2_sampler.sample(N)
        # rearrange data
        bc_x = np.concatenate([bc1_x, bc2_x])
        bc_u = np.concatenate([bc1_u, bc2_u])
        return torch.tensor(bc_x, dtype=torch.float32), torch.tensor(bc_u, dtype=torch.float32)

    def get_ic_batch(self, N):
        # sample points from initial condition
        ic_x, ic_u = self.ic_sampler.sample(N)
        return torch.tensor(ic_x, dtype=torch.float32), torch.tensor(ic_u, dtype=torch.float32)

    def get_test_batch(self, N):
        # sample batch for test
        # X_star size = [N, 1], velocity_ref size = [N, 1]
        nn = int(N ** (0.5))
        # generate mesh
        x1 = np.linspace(self.bound_l, self.bound_r, nn)[:, None]
        x2 = np.linspace(self.bound_d, self.bound_t, nn)[:, None]
        x1, x2 = np.meshgrid(x1, x2)
        x_star = np.hstack((x1.flatten()[:, None], x2.flatten()[:, None]))
        u_star = self.u(x_star)
        return (torch.tensor(x_star, dtype=torch.float32),
                torch.tensor(u_star, dtype=torch.float32))

    def get_img_batch(self, N):
        nn = int(N ** 0.5)
        # generate mesh
        x1 = np.linspace(self.bound_l, self.bound_r, nn)[:, None]
        x2 = np.linspace(self.bound_d, self.bound_t, nn)[:, None]
        x1, x2 = np.meshgrid(x1, x2)
        # rearrange data
        x_star = np.hstack((x1.flatten()[:, None], x2.flatten()[:, None]))
        u_star = self.u(x_star)
        return x1, x2, x_star, u_star


if __name__ == '__main__':
    # load device
    device = torch.device(f"cuda:0" if torch.cuda.is_available() else "cpu")
    print(f"Use GPU: {torch.cuda.is_available()}\n")
    # create save path
    save_path = f"../output/Klein-Gordon-a={a}/PIRBN/"
    log_path, model_path, pic_path = create_save_path(save_path)
    # load dataset
    dataset = DatasetKG(a, bound_l, bound_r, bound_t, bound_d)
    X_star, U_star = dataset.get_test_batch(10000)
    # create model
    network = NeuralNetwork(X_star, layers, device).to(device)
    pinn = PINNKG(network, dataset, batch_size, bound_weight, ic_weight, log_path, model_path, pic_path,
                           device)
    # train model
    pinn.train(lr_rate=1e-3, decay_factor=0.5, total_it=35010, print_it=10, evaluate_it=100, pic_it=1000, lr_it=10000)
